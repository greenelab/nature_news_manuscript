## Methods

### Data Acquisition and Processing

#### Text Scraping

We scraped all text and metadata from _Nature_ using the web-crawling framework Scrapy [@https://scrapy.org/] (version 2.4.1).
We created three independent scrapy web spiders to process the news text, news citations, and paper metadata.
News articles were defined as all articles from 2005 to 2020 that were designated as "News", "News Feature", "Career Feature", "Technology Feature", and "Toolbox".
Using the spider “target_year_crawl.py”, we scraped the title and main text from all news articles.
We character normalized the main text by mapping visually identical Unicode codepoints to a single Unicode codepoint and stripping many invalid Unicode characters.
Using an additional spider defined in “doi_crawl.py”, we scaped all citations within news articles.
For simplicity, we only considered citations with a DOI included in either text or a hyperlink in this spider.
Other possible forms of citations, e.g., titles, were not included.
The DOIs were then queried using the _Springer Nature_ API.
The spider “article_author_crawl.py” scraped all articles designated "Article" or "Letters" from 2005 to 2020.
We only scraped author names, author positions, and associated affiliations from research articles, which we refer to as _papers_.
It should be noted that "News" article designations changed over time.

#### _The Guardian_ API

To obtain science-related articles from _The Guardian_, we used their API which is available here: https://open-platform.theguardian.com/.
We queried for 100 articles in the "science" section per month from 2005 to 2020, which typically covered all available articles.
All html was removed using the R package textclean [@https://cran.r-project.org/web/packages/textclean/index.html].
Similar to the data scraped from _Nature_, we also mapped visually identical Unicode codepoints to a single Unicode codepoint and stripped all non-ASCII characters.
Since citations are hyperlinked in-line, we did not extract any citation information from _The Guaradian_.

#### coreNLP

After the news articles were scraped and processed, the text was processed using the coreNLP pipeline [@doi:10.3115/v1/P14-5010] (version 4.2.0).
The main purpose for using coreNLP was to identify named entities related to countries and quoted speakers.
The full set of annotaters were: tokenize, ssplit, pos, lemma,ner, parse, coref, quote.
We used the "statistical" algorithm to perform coreference resolution.
All results were output to json format for further downstream processing.


#### _Springer Nature_ API

_Springer Nature_ was chosen over other publishers for multiple reasons: 1) it is a large publisher, second only to Elsevier; 2) it covers multiple subjects, in contrast to PubMed; 3) its API has a large daily query limit (5000/day); and 4) it provided more author affiliation information than found in Elsevier. 
We generated a comparative background set for supplemental analysis with the _Springer Nature_ API by obtaining author information for papers cited in news articles.
We selected a random set of papers to generate the _Springer Nature_ background set.
These papers were the first 200 English language "Journal" papers returned by the _Springer Nature_ API for each month, resulting in 2400 papers per year for 2005 through 2020.
To obtain the author information for the cited papers, we queried the _Springer Nature_ API using the scraped DOI.
For both API query types, the author names, positions, and affiliations for each publication were stored and are available in "all_author_country.tsv" and "all_author_fullname.tsv".
 
### Name Formatting

#### Name Formatting for Gender Prediction in Quotes or Mentions
We first pre-filter articles that have more than 25 quotes, which is 1.86% (704/37,748) of total articles.
This was done to ensure no single article is over-represented and to avoid spuriously identified quotes due to unusual article formatting.
To identify the gender of a quoted or mentioned person, we first attempt to identify the person’s full name.
Even though genderizeR only uses the first name to make the gender prediction, identifying the full name gives us greater confidence that we are using the first name.
To identify the full name, we take the predicted speaker by coreNLP and match it to the longest matching name within the same article.
We match names by finding the longest mentioned name in the article with minimal edit (Levenshtein) distance.
The name with the smallest edit distance, where character deletions have zero cost, is defined as the matching name.
Character deletion was assigned a zero cost because we would like exact substring matches.
For example, the calculated cost, including a cost for character deletion, between John and John Steinberg is 10; without character deletion, it is 0.
Compared with the distance between John and Jane Doe, with character deletion cost, it is 7; without it is 2.
If we are still unable to find a full name, or if coreNLP cannot identify a speaker at all, we also determine whether or not coreNLP linked a gendered pronoun to the quote.
If so, we predict that the gender of the speaker is the gender of the pronoun.
We ignore all quotes with no name or partial names and no associated pronouns.
A summary of processed gender predictions of quotes at each point of processing is provided in Table {@tbl:table1}.


#### Name Formatting for Gender Prediction of Authors

Because we separate first and last authors, we only considered papers with more than one author.
As for quotes, we needed to extract the first name of the authors.
We cast names to lowercase and processed them using the R package humaniformat [@https://cran.r-project.org/web/packages/humaniformat/index.html].
humaniformat identifies if names are reversed (Lastname, Firstname), as well as identifies middle names.
This processing was not required for quote prediction because names written in news articles did not appear to be reversed or abbreviated.
Since many last or first authorships may be non-names, we additionally filtered out any identified names if they partially or fully match any of the following terms: "consortium", "group", "initiative", "team", "collab", "committee", "center", "program", "author", or "institute".
Furthermore, since many papers only contain first name initials (for example, "N. Davidson"), we remove any names less than four letters (length includes punctuation) and containing a "." or "-", then strip out all periods from the first name.
This ensures that hyphenated names are not changed, e.g. Julia-Louise remains unchanged, but removes hyphenated initials, e.g. J-L.
Finally, we only consider any remaining first names of more than two characters.
This is to eliminate first and middle jointly-initialized names.
For example, "NR Davidson" would be reduced to "Davidson" and then eliminated due to the lack of a first name.
A summary of processed author gender predictions at each point of processing is provided in Tables {@tbl:table2} - {@tbl:table4}.


#### Name Formatting for Name Origin Prediction
In contrast to the gender prediction, we require the entire name in all steps of name origin prediction.
For names identified in the _Nature_ news articles, we use the same process as described for the gender prediction; we again try to identify the full name.
For author names, we process the names as previously described for the gender prediction of authors.
For all names, we only consider them in our analyses if they consist of two distinct parts separated by a space.
Additionally, if a full name is less than three characters, we were unable to consider it as the prediction model that we apply uses 3-mers.
A summary of processed name origin predictions of quotes and citations at each point of processing is provided in Tables {@tbl:table1} - {@tbl:table4}.


### Gender Analysis

The quote extraction and attribution annotator from the coreNLP pipeline was employed to identify quotes and their associated speakers in the article text.
In some cases, coreNLP could not identify an associated speaker’s name but instead assigned a gendered pronoun.
In these instances, we used the gender of the pronoun for the analysis.
The R package genderizeR [@doi:10.32614/rj-2016-002], a wrapper for the genderize.io API [@https://genderize.io/], predicted the gender of authors and speakers.
We predicted a name as male using the first name with a minimum cutoff of 50%.
To reduce the number of queries made to genderize.io, a previously cached gender prediction from [@doi:10.1101/2020.04.14.927251] was also used and can be found in the file "genderize.tsv".
All first name predictions from this analysis are in the file "genderize_update.tsv".
To estimate the gender gap for the quote gender analyses, we used the proportion of total quotes, not quoted speakers.
We used the proportion of quotes to measure speaker participation instead of only the diversity of speakers.
The specific formulas for a single year are shown in equations @eq:quote and @eq:first-author.
We did not consider any names where no prediction could be made or quotes where neither speaker nor gendered pronoun was associated.

$$\textrm{Prop. Male Quotes} = \frac{|\textrm{Male Speaker Quotes}|} {|\textrm{Male or Female Speaker Quotes}|}$${#eq:quote}
 
$$\textrm{Prop. Male First Authors} = \frac{|\textrm{Male First Authors}|} {|\textrm{Male or Female First Authors}|}$${#eq:first-author}
 
### Name Origin Analysis

We used the same quoted speakers as described in the previous section for the name origin analysis.
In addition, we also consider all authors cited in a _Nature_ news article.
In contrast to the gender prediction, we need to use the full name to predict name origin.
We submitted all extracted full names to Wiki-2019LSTM [@doi:10.1101/2020.04.14.927251] to predict one of ten possible name origins: African, Celtic/English, East Asian, European, Greek, Hispanic, Hebrew, Arabic/Turkish/Persian, Nordic, and South Asian.
While a full description of Wiki-2019LSTM is outside the scope of this paper, we describe it here breifly.
Wiki-2019LSTM is trained on name and nationality pairs, using 3-mers of the characters in a name to predict a nationality.
To ensure robust predictions, nationalities were grouped together as described in NamePrism [@doi:10.1145/3132847.3133008].
NamePrism chose to exclude the United States, Australia, and Canada from their country groupings and were therefore excluded during training of Wiki-2019LSTM.
This choice was justified by NamePrism in stating that these countries had a high level of immigration.
The treemap of country groupings defined in the NamePrism manuscript are found in figure 5 of the publication [@doi:10.1145/3132847.3133008]. 

After running the pre-trained Wiki-2019LSTM model, we select the highest probability origin for each name as the resultant assignment.
Similar to the gender analyses, quote proportions were again directly compared against publication rates.
For citations, quotes, and mentions, we calculated the proportion for a given year for each name origin.
This is shown in @eq:cite-origin to, for example, calculate the citation rate for last authors with a Greek name origin for a single year.
 
$$\textrm{Prop. Greek Last Author Cited} = \frac{| \textrm{Cited Last Authors w/Greek Name}|} {| \textrm{Cited Last Authors w/any Name}|}$${#eq:cite-origin}

$$\textrm{Prop. Greek Quotes} = \frac{| \textrm{Quotes w/Greek Named Speaker}|} {| \textrm{Quotes w/any Named Speaker}|}$${#eq:quote-origin}

$$\textrm{Prop. Greek Names Mentioned} = \frac{| \textrm{Unique Greek Names Mentioned}|} {| \textrm{Unique Names w/any Origin Mentioned}|}$${#eq:quote-origin}



### Country Mention Proportions

We estimated the prevalence of a country’s mentions by including all identified organizations, countries, states, or provinces from coreNLP's named entity annotater.
We queried the resultant terms using OpenStreetMap [@https://www.openstreetmap.org] to identify the associated country with the term.
All terms that were identified in the text 25 or more times were visually inspected for correctness.
Hand-edited entries are denoted in the OpenStreetMap cache file "osm_cache.tsv" by the column "hand_edited".
Still, this only accounts for less than 5% of the total entries.
Furthermore, country-associated terms identified by coreNLP may be ambiguous, causing OpenStreetMap to return incorrect locations.
Therefore, we count country mentions only if we find at least two unique country-associated terms in an article.
We calculate the mentioned rate as the proportion of country-specific mentions divided by the total articles for a particular year, as exemplified in @eq:mention for calculating the mentioned rate for Mexico for a single year.
 
$$\textrm{Prop. Mexico Mentions} = \frac{|\textrm{Articles with} \geq 2 \textrm{ unique Mexico-related terms}|} {|\textrm{All News Articles}|}$${#eq:mention}
 
### Country Citation Proportions

To identify the citation rate of a particular country, we processed all authors’ affiliations for a specific article.
Since the affiliations could be in multiple formats, we again used OpenStreetMap to identify the country affiliation.
Additionally, we considered all affiliations for a single author.
We calculated a countries' citation rate as the number of citations for a country divided by either the number of _Nature_ papers (@eq:mention-bg) or the total number of papers cited by news articles for that year (@eq:mention-news).
Shown below are example calculations for Colombia for a single year.
 
$$\textrm{Prop. CO Affil. in Nature} = \frac{|\textrm{Articles with} \geq 1 \textrm{ CO affil. in Nature}|} {|\textrm{All Nature Research Articles}|}$${#eq:mention-bg}
 
$$\textrm{Prop. CO Affil. Citations} = \frac{|\textrm{Cited Articles in News with} \geq 1 \textrm{ CO affil.}|} {|\textrm{All Articles Cited in News}|}$${#eq:mention-news}
 
### Divergent Word Identification

As mentioned in our previous description of how we utilized _The Guardian_ API, no citations were extracted from _The Guardian_, only _Nature_ articles were used in this analysis.
After calculating the citation and mention proportion for each country, we identified countries outlying in their comparative citation or mention rate.
Outlier detection was done by subtracting the citation and mention rates, then identifying which countries were in the top or bottom 5% from each year.
We only considered countries identified as either high citation (Set C) or high mention (Set M) across all years.
We did not consider any country that was in the top and bottom 5% in different years.
Additionally, we only considered a country if cited or mentioned five times in a single year.
Once we identified the set of C and M countries, we analyzed the word frequencies in all news articles where the set C or M country was mentioned but not cited.
We believe this would provide insight into content differences between set C and M countries.
Text from articles in 2020 were not considered due to an excess of SARS-CoV-2 related terms.
Using the R package tidytext [@https://cran.r-project.org/web/packages/tidytext/index.html] we extracted tokens, removed stop words, and calculated the token frequencies across all articles.
We only consider tokens in set C or M articles if the token has been observed at least 100 times across all articles.
We then identify tokens that have the most significant ratio of usage between the two sets.
Since there are differences in the number of articles per country within each set, we calculated a token frequency within a set as the median frequency within each countries associated articles.
We calculated the resultant token ratio as the country normalized citation frequency to the country normalized mention frequency.
To avoid divide by zero errors, a pseudocount of 1 is added to both the numerator and denominator.
We assert that the term must be observed at least once in each set.

### Bootstrap Estimations
 
For all analyses related to equations @eq:quote - @eq:mention-news, we independently selected 5000 bootstrap samples for each year.
We sampled with replacement of size equal to the cardinality of the complete set of interest.
Bootstrap estimates for equations @eq:quote - @eq:mention-news were performed by sampling the denominator set.
The mean, 5th, 95th quantiles across the estimates are reported as the estimated mean, lower, and upper bounds.
For the divergent word analysis, due to computational constraints, we only took 1000 bootstrap samples.
The bootstrap estimates were taken by subsampling the news articles with replacement, each time recalculating the country-normalized token frequencies within each country set (C and M).
After the normalized frequencies within each country set were calculated, we calculated the ratio between country sets for each subsample with a pseudocount of 1 in the numerator and denominator, (C+1)/(M+1).
Again, the mean, 5th, 95th quantiles across the estimates are reported as the estimated mean, lower, and upper bounds.

